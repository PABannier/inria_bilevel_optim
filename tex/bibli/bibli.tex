\documentclass[a4paper,10pt]{article}
\usepackage[margin=2.5cm]{geometry}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{bm}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithms
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[titlenumbered,ruled,noend,algo2e]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetEndCharOfAlgoLine{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{fancyvrb}                  % for fancy verbatim
\usepackage{textcomp}
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}
\usepackage[procnames]{listings}
% \usepackage{setspace} % need for \setstretch{1}
\lstset{%
language   = python,%
 % basicstyle = \ttfamily\setstretch{1},%
basicstyle = \ttfamily,%
columns    = flexible,%
keywordstyle=\color{javared},
firstnumber=100,
frame=shadowbox,
showstringspaces=false,
morekeywords={import,from,class,def,for,while,if,is,in,elif,
else,not,and,or,print,break,continue,return,True,False,None,access,
as,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert,!=},
keywordstyle={\color{javared}\bfseries},
commentstyle=\color{javagreen}, %vsomb_col white comments
morecomment=[s][\color{javagreen}]{"""}{"""},
upquote=true,
%% style for number
numbers=none,
resetmargins=true,
xleftmargin=10pt,
linewidth= \linewidth,
numberstyle=\tiny,
stepnumber=1,
numbersep=8pt, %
frame=shadowbox,
rulesepcolor=\color{black},
procnamekeys={def,class},
procnamestyle=\color{oneblue}\textbf,
literate={á}{{\'a}}1
{à}{{\`a }}1
{ã}{{\~a}}1
{é}{{\'e}}1
{ê}{{\^e}}1
{è}{{\`e}}1
{í}{{\'i}}1
{î}{{\^i}}1
{ó}{{\'o}}1
{õ}{{\~o}}1
{ô}{{\^o}}1
{ú}{{\'u}}1
{ü}{{\"u}}1
{ç}{{\c{c}}}1
}


\usepackage{times} % use Times

\usepackage{shortcuts_js} % possibly adapted from https://github.com/josephsalmon/OrganizationFiles/sty/shortcuts_js.sty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use prebuiltimages/ for images extracted from code (e.g. python)
% or to share images built from a software not available by the whole team (say matlab .fig, or inskcape .svg).
% .svg files should be stored in dir srcimages/ and built from moosetex if needed:
% https://www.charles-deledalle.fr/pages/moosetex.php
% NEVER (GIT) versions files in images/ : only prebuiltimages/ & srcimages/ !

\usepackage{graphicx} % For figures
\graphicspath{{images/}, {prebuiltimages/}}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[authoryear]{natbib}
\usepackage{cleveref} % mandatory for no pbs with hyperlinks theorem etc...
\crefformat{equation}{Eq.~(#2#1#3)} % format for equations
\Crefformat{equation}{Equation~(#2#1#3)} % format for equations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header and document start
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{Pierre-Antoine}
\title{Internship - Bibliography}

\begin{document}

\maketitle

\vskip 0.3in

\section{Hyperparameter optimization with approximate gradient, Pedregosa et al.}

Hyperparameters have a significant impact on model predictive capability. Finding the right set
of hyperparameters $\lambda \in \mathbb{R}^s$, where $s \in \mathbb{N}^*$ is the number of hyperparameters
is mostly an empirical process that involves many iterations and does not follow a clearly-defined pathway
but rather some general guidelines.

Being able to derive an automatic procedure that finds the optimal set of hyperparameters (in terms of generalization
ability, usually measured using a cross-validated metric like AIC/BIC or SURE\footnote{For more information, see \cite{Deledalle_Vaiter_Fadili_Peyre14}}) is an active field of research.

\subsection*{Why can't we directly optimize the loss function by including the hyperparameter as the parameter in the loss?}

Because without imposing a constraint (inner loss), the optimization algorithm would always pick the model with no regularization.
We need to come up with a way to cross-validate the choice of hyperparameters.
\\
\\
In the rest of the section, $f$ is a function with Lipschitz gradient that depends on the model
parameters that we denote $X(\lambda)$, where $\lambda \in \mathbb{R}^s$ is a vector of hyperparameters.
Those model parameters are typically not available in closed form but can be written as minimizing quantities of a cost function $h(\cdot, \lambda) : \mathbb{R}^p \rightarrow \mathbb{R}$,
where $p$ is the number of parameters in a model. This enables us to define two nested optimization problems, that we call
\textbf{bi-level optimization problem}.

\begin{align*}
    \argmin_{\lambda \in D} \Bigl\{ f(\lambda) \triangleq g(X(\lambda), \lambda) \Bigr\} \\
    \text{s.t.} \quad X(\lambda) \in \argmin_{x \in \mathbb{R}^p} h(x, \lambda)
\end{align*}
\\
The constraint is called the \textbf{inner} optimization problem, while the minimizing quantity is called the \textbf{outer} optimization problem.

\subsection*{What are the three paradigms of hyperparameter optimization?}

\begin{itemize}
    \item \textbf{Random and grid search (zero order)}
    \item \textbf{Sequential model-based optimization (zero order)}: Bayesian optimization essentially consists in evaluating the function in multiple points in the parameter space, and fit a cheap proxy model
                  (Gaussian processes, kernel methods\dots) to iteratively estimate the distribution of hyperparameters. For more information, see \cite{Brochu_Cora_deFreitas10}.
    \item \textbf{Gradient-based hyperparameter optimization (first order)}: Use local information about the cost function to make informed decision
                  in the optimization process. It consists in evaluating the hypergradient (the gradient of the loss \textit{w.r.t.} the hyperparameters.)
\end{itemize}

The major bottleneck (in terms of speed) in first-order methods is to compute the hypergradient.
Therefore, this paper proposes a first order hyperparameter optimization method based on noisy approximations of the hypergradient.

\subsection*{How to compute the hypergradient in closed-form?}

One approach called \textbf{implicit differentiation} consists in computing the hypergradient in closed-form by differentiating with respect to $\lambda$.

\begin{align*}
    \nabla f &= \nabla g(X(\lambda), \lambda) \\
             &= \nabla_2 g + DX \cdot \nabla_1 g
\end{align*}
\\
Yet, $X(\lambda) \in \argmin_{x \in \mathbb{R}^p} h(x, \lambda)$, so $\nabla_1 h(X(\lambda), \lambda)=0$. Differentiating this equation with respect to $\lambda$
gives: $\nabla_{1, 2}^2 h + \nabla^2_1 h \cdot DX = 0$, therefore $DX = - \nabla^2_{1,2}h \cdot (\nabla_1^2 h)^{-1}$.  Finally, using the previous equation with
the definition of $DX$ yields:

\begin{equation*}
    \nabla f = \nabla_2 g - \nabla^2_{1,2}h \cdot (\nabla_1^2 h)^{-1} \cdot \nabla_1 g
\end{equation*}
\\
In practice, this expression is cumbersome, hence difficult to evaluate. The contribution of this paper is to compute an approximation of the hypergradient using noisy estimates
of the gradients and the Hessians of $h$ and $g$. In particular, $\nabla_2 g$, $\nabla^2_1 h$ can be
easily computed in closed form for cheaper models (although in practice we prefer to evaluate
a matrix-vector product than computing the full Hessian \footnote{In the OLS case for instance, $\mathbf{H} = \mathbf{X^\top X}$. If $\mathbf{X} \in \mathbf{R}^{n \times p}$,
$\mathbf{H} \in \mathbb{R}^{p \times p}$, which can be expensive to evaluate. We prefer for $v \in \mathbb{R}^{p}$ to evaluate
first $y=Xv$ then $z=\mathbf{X^\top}y$}) using a conjugate-gradient method.
\\
\\
Besides, the hypergradient can be computed using iterative differentiation.

\subsection*{Why choosing projected gradient descent over gradient descent?}

Gradient descent is a standard (easy and simple) way to solve unconstrainted optimization
problem like $\min_{x \in \bbR^n} f(x)$. However, as soon as we impose a constraint over
the feasible set like $\min_{\norm{x}_2 \leq 1} \norm{\mathbf{Ax-b}}_2^2$, gradient descent is no longer suited for such problems.
\\
\\
We need to rely on a modified version of gradient descent: $\textbf{projected gradient descent (PGD)}$. PGD adds an addtional step
in the GD algorithm: the projection step. Let $P_{\mathcal{Q}}$ be a projection operator on some open set $Q$:

\begin{equation*}
    P_{\mathcal{Q}}(x_0) = \frac{1}{2}\argmin_{x \in \mathcal{Q}} \norm{x - x_0}_2^2
\end{equation*}

And the projected gradient descent step reads:

\begin{equation*}
    x_{k+1} = P_{\mathcal{Q}}(x_k - \alpha \nabla f(x_k))
\end{equation*}

We can convince ouverselves that if $\mathcal{Q}$ is a convex set, the solution to the projection problem is unique. Otherwise, it may
not be unique. If the next point after a gradient descent step is in $\mathcal{Q}$, the projected point is the point itself. But if $x$
does not lie in $\mathcal{Q}$, then the projected point lies on the boundary of $\mathcal{Q}$.
\\
\\
Also, note that projected gradient descent is a special case of proximal gradient descent where the
non-differentiable function is the indicator function.


\section{Implicit differentiation for fast hyperparameter selection in non-smooth convex learning, Bertrand and al.}

The bilevel optimization problem is the problem consisting in optimizing the loss function w.r.t. the hyperparameters (outer loss)
with respect to a constraint: minimizing the criterion w.r.t. the parameters  of an estimator (inner loss).
One \textit{strong} assumption: the regularization path is well-defined and almost everywhere differentiable.\\

3 algorithms to compute hypergradients:
\begin{itemize}
    \item Implicit differentiation
    \item Forward auto differentiation
    \item Backward auto differentiation (backprop)
\end{itemize}

\subsection*{Contributions}

\begin{itemize}
    \item There exist methods to efficiently compute hypergradients for non-smooth functions.
    \item Leveraging the sparsity of the Jacobian matrix, we propose an efficient implicit differentiation algorithm to compute the hypergradients.
    \item Implicit differentiation significantly outperforms forward and backward auto differentiation.
\end{itemize}

\vskip 0.1in

Note that in practice, (proximal) coordinate descent is much faster for solving LASSO-like problems than accelerated (proximal) gradient descent à la Nesterov.
For more details, see \cite{Bertrand_Massias_Anderson}.

\section{Enhancing Sparsity with Reweighted l1 minimization, Candès and al.}

See memo on LASSO.


\newpage
\bibliographystyle{plainnat}
\bibliography{references_all}

\end{document}
