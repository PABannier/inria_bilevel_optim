\documentclass[a4paper,10pt]{article}
\usepackage[margin=2.5cm]{geometry}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{bm}




\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cross-referencing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithms
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[titlenumbered,ruled,noend,algo2e]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetEndCharOfAlgoLine{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{fancyvrb}                  % for fancy verbatim
\usepackage{textcomp}
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}
\usepackage[procnames]{listings}
% \usepackage{setspace} % need for \setstretch{1}
\lstset{%
language   = python,%
 % basicstyle = \ttfamily\setstretch{1},%
basicstyle = \ttfamily,%
columns    = flexible,%
keywordstyle=\color{javared},
firstnumber=100,
frame=shadowbox,
showstringspaces=false,
morekeywords={import,from,class,def,for,while,if,is,in,elif,
else,not,and,or,print,break,continue,return,True,False,None,access,
as,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert,!=},
keywordstyle={\color{javared}\bfseries},
commentstyle=\color{javagreen}, %vsomb_col white comments
morecomment=[s][\color{javagreen}]{"""}{"""},
upquote=true,
%% style for number
numbers=none,
resetmargins=true,
xleftmargin=10pt,
linewidth= \linewidth,
numberstyle=\tiny,
stepnumber=1,
numbersep=8pt, %
frame=shadowbox,
rulesepcolor=\color{black},
procnamekeys={def,class},
procnamestyle=\color{oneblue}\textbf,
literate={á}{{\'a}}1
{à}{{\`a }}1
{ã}{{\~a}}1
{é}{{\'e}}1
{ê}{{\^e}}1
{è}{{\`e}}1
{í}{{\'i}}1
{î}{{\^i}}1
{ó}{{\'o}}1
{õ}{{\~o}}1
{ô}{{\^o}}1
{ú}{{\'u}}1
{ü}{{\"u}}1
{ç}{{\c{c}}}1
}


\usepackage{times} % use Times

\usepackage{shortcuts_js} % possibly adapted from https://github.com/josephsalmon/OrganizationFiles/sty/shortcuts_js.sty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use prebuiltimages/ for images extracted from code (e.g. python)
% or to share images built from a software not available by the whole team (say matlab .fig, or inskcape .svg).
% .svg files should be stored in dir srcimages/ and built from moosetex if needed:
% https://www.charles-deledalle.fr/pages/moosetex.php
% NEVER (GIT) versions files in images/ : only prebuiltimages/ & srcimages/ !

\usepackage{graphicx} % For figures
\graphicspath{{images/}, {prebuiltimages/}}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[authoryear]{natbib}
\usepackage{cleveref} % mandatory for no pbs with hyperlinks theorem etc\dots
\crefformat{equation}{Eq.~(#2#1#3)} % format for equations
\Crefformat{equation}{Equation~(#2#1#3)} % format for equations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header and document start
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{Pierre-Antoine Bannier}
\title{A quick guided tour of LASSO-like models and bi-level optimization}

\begin{document}

\maketitle

\vskip 0.3in

In this memo, $\mathbf{X} \in \bbR^{n \times p}$ is the design matrix, $\beta \in \bbR^{p}$ (resp. $\mathbf{B} \in \bbR^{p \times T}$) is the coefficient vector (resp. matrix) and $y \in \bbR^n$
(resp. $\mathbf{Y} \in \bbR^{n \times T}$) is the target vector (resp. matrix). Let $n, p, T \in \mathbb{N}^*$ be respectively the number of samples, the number of features and the number of tasks. As we shall see later, LASSO-like models
are particularly useful for large-dimensional problems \textit{i.e.} when $p \gg n$. $\lambda \in \bbR^+$ is a regularizing hyperparameter. We write $\bbS^n$ for the set of symmetric matrices and $\bbS^n_+$ (respectively $\bbS^n_{++}$) for
the set of positive semi-definite (respectively positive definite) matrices.


\section{LASSO}
\label{section_1}

In this section, we assume that $T = 1$ and $p \gg n$.
\\
LASSO consists in solving the following optimization problem:

\begin{equation*}
    \beta^* = \argmin_{x \in \bbR^{p}} \frac{1}{2n}\norm{ y - \mathbf{X} \beta}_2^2
    + \lambda \norm{ \beta }_{1}
    \enspace .
\end{equation*}
%
LASSO is a useful estimator to reconstruct sparse signals. It acts as a convex regularizer surrogate for the $l_0$ norm. Indeed, the
$l_0$ norm yields a $NP$-hard combinatorial non-convex problem to solve. \\

\subsection*{Why do we want sparsity?}

For large-dimensional problems, there exists an infinite set of solutions (overcomplete set).
Imposing a structure constraint on the solution $\beta$ is critical to find a unique solution to the problem.

\subsection*{How is this problem solved?}

Unlike unregularized OLS problem, the LASSO estimator does not generally have a closed form solution. Therefore, we need to rely on optimization methods for solving this optimization problem. Actually, this convex problem
can be recast as a linear pogram. In practice, \textbf{proximal coordinate descent} has proved to be the most efficient method for solving such problems, see \cite{Bertrand_Massias_Anderson}.

\section{Group LASSO}
\label{section_2}

There are many regression problems where the features have a natural group structure, and it is desirable to have all coefficients within a group become nonzero or zero simulatenously.
\\
Consider a partitioning of the features such that we can partition the columns of $\mathbf{X}$ as $\{x_{1}, \dots, x_{J}\}$ where $x_{j} \in \bbR^{n \times p_j}$ and
$p_j$ is the number of features in group $j$. We can apply the same partition to the coefficients of the coefficient vector $\beta \in \bbR^p$. Indeed, $\beta$ is of the form $(\beta_{1}, \dots, \beta_{j})$ where
$\beta_{j} \in \bbR^{p_j}$. Note that we have as many unique coefficients in $\beta$ as there are groups in the partition of the features in the design matrix. Then, the optimization problem reads:

\begin{equation*}
    \beta^* = \argmin_{\beta \in \bbR^p}
        \left\{
            \frac{1}{2n} \sum_{i=1}^n (y_i - \sum_{j=1}^J x_{j}^\top \beta_{j})^2
            + \lambda \sum_{j=1}^{J} \norm{ \beta_{j}}_2
        \right\}
    \enspace .
\end{equation*}
%
$ \norm{\beta_{j}}_2$ is the Euclidean norm of the vector $\beta_{j}$. We can convince ourselves that if $\forall j \in \{1, \dots, J\}, p_j = 1$, then $\norm{ x_{j}}_2 = \lvert x_{j} \rvert$,
thus the problem reduces to \nameref{section_1}.

\subsection*{Why is this $l_1 / l_2$-mixed penalty still inducing sparsity?}

The penalty set on the optimization problem is called a $l_1/l_2$-mixed norm. First, we are computing the $l_2$ norm of the coefficients in one group, and then we are summing all of them.
Simply put, the $l_1/l_2$-mixed norm can be seen as the $l_1$ norm of the $l_2$ norms, which means that there will be few non-zero $l_2$ norms, which in turn creates group sparsity.
Indeed, remember that since $\lvert \lvert \cdot \rvert \rvert_2$ is a norm, for any vector $x$, $\norm{ x}_2 = 0 \Rightarrow x = 0$.

\subsection*{How is this optimization problem solved?}

This problem is again solved using proximal coordinate descent. However, we are no longer iterating over features one by one but group by group, leading to the \textbf{proximal block coordinate descent}.

\section{Adaptive/Weighted LASSO}
\label{section_3}

Recall that the $l_1$ norm is a convex surrogate of the $l_0$ norm. However, it has some limitations compared to
the $l_0$-norm. Indeed, the $l_1$-norm is undemocratic in that it penalizes more heavily coefficients larger coefficients than smaller ones. It turns out that the LASSO penalty exerts less pressure on small coefficients,
thus letting them being small while not setting them to zero. In this vein, the adaptive LASSO has been proposed as a way for fitting models sparser than \nameref{section_1}, by introducing weights in front of every coefficient that is
penalized. The optimization problem reads:

\begin{equation*}
    \beta^* = \argmin_{\beta \in \bbR^p}
        \left\{
            \frac{1}{2n} \norm{ y - \mathbf{X}\beta}_2^2
            + \lambda \sum_{j=1}^p w_j \lvert \beta_j \rvert
        \right\}
    \enspace .
\end{equation*}
%
where $w \in \bbR^p$ is a weight vector. By placing $w \in \bbR^p$ on the diagonal of a square matrix $\mathbf{W} \in \bbR^{p \times p}$, we can rewrite the optimization problem in a matrix form:

\begin{equation*}
    \beta^* = \argmin_{x \in \bbR^p}
        \left\{
            \frac{1}{2n} \norm{y - \mathbf{X}\beta}_2^2
            + \lambda \norm{\mathbf{W}\beta}_1
        \right\}
    \enspace .
\end{equation*}
%
\subsection*{How to choose the weight vector?}

The rationale is that large weights could be used to discourage nonzero entries in the recovered signal, while small weights would encourage them. As a rule of thumb, the weights should relate inversely
to the true signal magnitude (in the context of signal processing, which is of interest here). However, we don't have access to the true signal magnitudes. The question remains how a valid set of weights
may be obtained without first knowing the true signal. \cite{Candes_Wakin_Boyd08} argues that there may exist a range of favorable weighting matrices $\mathbf{W} \in \bbR^{p \times p}$.
\\
\\
They propose a majorization-minimization (MM) algorithm to iteratively reweight the coefficients of the LASSO estimator.
Recall that a MM algorithm works by iteratively minimizing a surrogate function that majorizes the objective function.
Simply put, first we majorize the objective function by a surrogate, then we minimize this surrogate function \footnote{It is a core idea in optimization, used to derive gradient descent for instance.
For a convex $L$-smooth function, we majorize it by its quadratic upper bound, then we minimize this quadratic upper bound.
The quadratic upper bound is easily minimized since it has a closed-form solution.
For more details, see \url{http://josephsalmon.eu/enseignement/UW/STAT593/MajorizationMinimization.pdf}}.
\\
Indeed, consider the following constrained problem:

\begin{equation*}
    \min_{\beta \in \bbR^p}
    \quad \sum_{i=1}^p \log(\lvert \beta_i \rvert + \epsilon)
    \quad \textrm{s.t.} \quad  y = \mathbf{X} \beta
    \enspace .
\end{equation*}
%

Clearly this problem is equivalent to the following:
\begin{equation*}
    \min_{\beta,u \in \bbR^p}
    \quad \sum_{i=1}^p \log(u_i + \epsilon)
    \quad \textrm{s.t.} \quad  y = \mathbf{\Phi} \beta,
    \quad \lvert \beta_i \rvert \leq u_i, \enspace i=1,\dots,p
    \enspace .
\end{equation*}
%
This problem is easier to solve since we get rid of the absolute value in the minimized term as it is now placed as an additional constraint.
To apply a MM algorithm, we need to majorize the log-sum function. Yet, the log-sum function is concave and therefore below its tangent. Thus by taking a first-order Taylor expansion, we obtain a linearization of
the log-sum function in a neighborhood of $u$.
\\
\\
More formally, let $g(u) = \sum_{i=1}^p \log(u_i + \epsilon)$. The first-order Taylor expansion in a neighborhood of $u^{(l)} \in \bbR^p$ yields:

\begin{align*}
    u^{(l+1)} &= \argmin_{u \in \bbR^p} g(u^{(l)}) + \nabla g(u^{(l)})\cdot (u - u^{(l)}) \\
              &= \argmin_{u \in \bbR^p} \sum_{i=1}^p \log( u_i^{(l)} + \epsilon) + (u - u^{(l)}) \sum_{i=1}^p \frac{1}{ u_i^{(l)}  + \epsilon}
    \enspace .
\end{align*}
%
By removing the terms that do not depend on $u$, it follows that:

\begin{equation*}
    u^{(l+1)} = \argmin_{u \in \bbR^p}
    \sum_{i=1}^p \frac{u_i}{ u_i^{(l)} + \epsilon}
    \enspace .
\end{equation*}
%
And by equivalence,

\begin{equation*}
    \beta^{(l+1)} = \argmin_{\beta \in \bbR^p}
    \sum_{i=1}^p \frac{\lvert \beta_i \rvert}{\lvert \beta_i^{(l)} \rvert + \epsilon}
    \enspace .
\end{equation*}
%
By letting $\forall i \in \{1, \dots, p\}, w_i^{(l)} = \frac{1}{\lvert \beta_i^{(l)}\rvert + \epsilon}$, it follows:

\begin{equation*}
    \beta^{(l+1)} = \argmin_{\beta \in \bbR^p}
    \sum_{i=1}^p w_i^{(l)} \lvert \beta_i \rvert
    \enspace .
\end{equation*}
%
Therefore, it follows an algorithm to iteratively reweight the coefficients of the LASSO model.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative reweighted l1 minimization}
}
%
\Input{$
    \mathbf{X} \in \bbR^{n \times p},
    \beta \in \bbR^{p},
    y \in \bbR^{n},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+,
    \lambda \in \bbR^+
    $}

\Init{$w = \ind_{\bbR^p} $}

    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $\beta^{(l)} \leftarrow \argmin \norm{ y -  \mathbf{X} \beta}_2^2 + \lambda \norm{ w \cdot \beta}_1$

        Update the weights: for each
        $i = 1, \dots, p, \enspace w_i^{(l+1)} \leftarrow \frac{1}{\lvert \beta_i^{(l)} \rvert + \epsilon}$
    }

\Return{$\beta$}
\end{algorithm}
}

\vskip 0.2in

Note that this algorithm minimizes a concave objective (log-sum is concave), by iteratively solving convex subproblems. Hence, due to the concavity of the objective function, we
are not guaranteed to converge to a global minimum.

\subsection*{Why choosing the log-sum function? How to choose $\epsilon$?}

The log-sum penalty function has the potential to be more sparsity-encouraging than the $l_1$ norm (for a visual explanation, see \cite{Candes_Wakin_Boyd08}). We clearly see
that the log-sum penalty is closer from the $l_0$ norm than the $l_1$ norm thus encouraging smaller coefficients to be set to zero.
\\
More precisely, the smaller $\epsilon$ the closer the log-sum function is from the $l_0$ norm. However, $\epsilon$ can't be set arbitrarily small since as $\epsilon \rightarrow 0$ it
becomes more likely that the iterative reweighted $l_1$ algorithm will be stuck at a local optimum. In practice, note that signal reconstruction is robust to the choice of $\epsilon$.

\section{Multi-Task LASSO}
\label{section_4}

In this section, we assume that $T > 1$. Therefore, we are now predicting a multivariate response $\mathbf{Y}\in \bbR^{n \times T}$, and by assuming that the data collected are linear measurements, the model reads:

\begin{equation*}
    \bf Y = XB + E
    \enspace .
\end{equation*}
%
where $\mathbf{E} \in \bbR^{n \times T}$ is a matrix of errors (noise), usually simulated with a pre-defined signal-to-noise ratio, when doing compressive sensing. Note that $\mathbf{B} \in \bbR^{p \times T}$ is a matrix.
Like in \nameref{section_2}, suppose that there exists an unknown subset $S \subset  \{1, \dots, p\}$ of the features that are relevant for prediction, and that \textbf{this same subset is preserved across all K components} of the response
variable. In this case, it is natural to consider a group LASSO penalty, in which the $p$ groups are defined by the rows $\{\beta_{j} \in \bbR^T, j=1,\dots,p\}$ of $\bf B$.
Therefore, the optimization problem we need to solve is:

\begin{equation*}
    \mathbf{B}^* = \argmin_{\mathbf{B} \in \bbR^{p \times T}}
        \left\{
            \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{X B}}_{\text{F}}^2
            + \lambda \sum_{j=1}^p \norm{ \beta_{j}}_2
        \right\}
    \enspace .
\end{equation*}
%
Using the $l_1/l_2$ mixed-norm, it reads:

\begin{equation*}
    \mathbf{B}^* = \argmin_{\mathbf{B} \in \bbR^{p \times T}}
        \left\{
            \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{XB}}_{\text{F}}^2
            + \lambda \norm{ \mathbf{B}}_{2, 1}
        \right\}
    \enspace .
\end{equation*}
%
Note that the fitting term is now a matrix norm (here the Frobenius norm) since we are dealing with matrices. As stated in \nameref{section_2}, we are using a mixed $l_1/l_2$ norm to enforce group sparsity on the coefficients $\mathbf{B}$. Indeed,
we are first computing the $l_2$ norm of every line of the coefficient matrix, then we are computing the $l_1$ norm of the remaining vectors full of the $l_2$ norm of the line of the coefficient matrix.

\subsection*{How is this optimization problem solved?}

In scikit-learn, the algorithm used to fit the model is coordinate descent.

\section{Multi-Task Reweighted LASSO}
\label{section_5}

The Multi-Task Reweighted LASSO is a combination of \nameref{section_3} and \nameref{section_4}. The goal of the model is to predict a multivariate response variable while enforcing a sparser solution than \nameref{section_1}.
\\
To induce a group-sparsity penalty like in \nameref{section_2}, we compute the $l_2$ norm of the lines of the coefficient matrix $\mathbf{B} \in \bbR^{p \times T}$, then we apply like in \nameref{section_3} a weighting matrix
on the resulting vector.
\\
Therefore, the optimization problems reads:

\begin{equation*}
    \mathbf{B}^* = \argmin_{\mathbf{B} \in \bbR^{p \times T}}
        \left\{
            \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{XB}}_{\text{F}}^2
            + \lambda \sum_{j=1}^p \underbrace{g(\norm{\mathbf{B}_{j:}}_{2})}_{g(\beta) \leq g(\beta^{(k)}) + \langle \nabla g (\beta^{(k)}) , \beta - \beta^{(k)} \rangle }
        \right\}
    \enspace .
\end{equation*}
%
where $g$ is any penalty convex or non-convex. In \cite{Candes_Wakin_Boyd08}, $g(\beta) = \sum_{i=1}^p \log(\lvert \beta_i \rvert + \epsilon)$. Algorithm 2 shows the log-concave reweighted $l_1$ minimization.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative Multi-task reweighted l1 minimization}
}
%
\Input{$
    \mathbf{X} \in \bbR^{n \times p},
    \mathbf{B} \in \bbR^{p\times T},
    \mathbf{Y} \in \bbR^{n\times T},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+_*,
    \lambda \in \bbR^+
    $}
    \Init{$w = \ind_{\bbR^p} $}

    % Fill  with ones \\
    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $\mathbf{B} \leftarrow \argmin \norm{\mathbf{Y} -  \mathbf{XB}}_{\text{F}}^2 + \lambda \norm{\text{diag}(w) \cdot \mathbf{B}}_{2, 1}$

        Update the weights: for each
        $j = 1, \dots, p, \enspace w_j \leftarrow \frac{1}{\norm{\mathbf{B}_{j:}}_2 + \epsilon}$
    }

\Return{$\mathbf{B}$}
\end{algorithm}
}

\newpage

\section{Bi-level optimization for structured sparse models using SURE}
\label{section_6}

In the following we refer as bi-level optimization the nested optimization problem
which consists in optimizing a criterion (outer loss) with respect to a hyperparameter, subject to
a second optimization problem (inner loss). More formally,

\begin{align*}
    &\lambda^* = \argmin_{\lambda \in \bbR^p} \Biggl\{\mathcal{L}(\lambda) \triangleq \mathcal{C}(\hat{\mathbf{X}}^{(\lambda)})\Biggr\} \\
    &\text{s.t.} \quad \hat{\mathbf{X}}^{(\lambda)} \in \argmin_{\mathbf{X} \in \bbR^{p\times T}} L(\mathbf{X}, \lambda)
\end{align*}
%
where $\mathcal{L}$ is the outer loss and $L$ is the inner loss.

Let's consider the \nameref{section_4} case. Consider a noisy data generating process such that:

\begin{equation*}
    \bf Y = XB_0 + E
    \enspace .
\end{equation*}
%
where entries of $\mathbf{E}$ are independent, centered and normally distributed.
For a more formal formulation of the multi-task regression problem, see \cite{Massias_Fercoq_Gramfort_Salmon17}. We wish to estimate
$\mathbf{B}_0$ such that we minimize:

\begin{equation*}
    \text{MSE}(\hat{\mathbf{B}}) = \bbE_{\mathbf{E}}\Biggl[ \norm{\mathbf{B}_0 - \hat{\mathbf{B}}}_{\text{F}}^2 \Biggr]
    \enspace .
\end{equation*}
%
However, the mean squared error can't be evaluated directly since it depends on $\bf B_0$, which is unknown
%
\footnote{Recall that MSE is a general concept: take the average of the squared differences between two quantities.
For more clarity, we can differentiate estimation error and prediction error. For two output vectors $y$ and $\hat{y}$,
we can easily evaluate $\text{MSE}(y, \hat{y})$ (prediction error). Conversely, evaluating the estimation error for one
true quantity and its estimator $\text{MSE}(\theta, \hat{\theta})$ is often impossible, hence the need for
an estimator of the quadratic risk, also called MSE.}.
%
Therefore, we need to find an estimator of the quadratic risk that is differentiable (to be optimized using first-order methods). Charles Stein has
proposed an unbiased estimator of the MSE, called the SURE (Stein's unbiased risk estimator) which reads:

\begin{equation*}
    \text{SURE}(\hat{\bf{B}}) =
        -nT\sigma^2 + \norm{\mathbf{Y}
        - \mathbf{\Phi} \hat{\mathbf{B}}}_{\text{F}}^2
        + 2 \sigma^2 \text{df}(\bf{\hat{B}})
    \enspace .
\end{equation*}
%
where $\text{df}(\mathbf{\hat{B}})$ is the number of degrees of freedom of $\hat{\mathbf{B}}$, alternatively called the divergence of
the estimator of $\mathbf{B}_0$. The two first terms are trivially evaluated, while the final term presents some
peculiarities hence the need for Stein's lemma
%
\footnote{For a proof of Stein's lemma, see:
\url{http://www.stat.cmu.edu/~larry/=sml/stein.pdf}}.
%
As we shall see below, $\text{SURE}$ is a differentiable estimator of $\text{MSE}$ that needs to verify some
non-trivial assumptions, the most notable of which being that \textbf{the estimator must be weakly differentiable with
respect to its target vector}.

\subsection*{Deriving the degrees of freedom in \nameref{section_4}}

Before diving into the derivation of the degrees of freedom in \nameref{section_4}, let's motivate the choice of $\text{SURE}$ as a criterion
to optimize. Remember that in a bi-level optimization setting, we can use $\text{SURE}$ for model selection purposes. LASSO-like
models depend on a tuning parameter $\lambda \in \mathcal{D}$ where $\mathcal{D}$ is some open domain then we can choose this parameter to
minimize $\text{SURE}$:

\begin{equation*}
    \hat{\lambda} = \argmin_{\lambda \in \mathcal{D}} \norm{ \mathbf{Y} - \mathbf{X}\hat{\mathbf{B}}^{(\lambda)} }_{\text{F}}^2
    + 2 \sigma^2 \text{df}(\hat{\bf{B}}^{(\lambda)})
    \enspace .
\end{equation*}
%
Note that the $\text{df}$ term refers to the degrees of freedom of the model, a measure used to quantify the complexity of a statistical modeling procedure.
\\
\\
First we need to verify that $\hat{\bf{B}}$ is weakly differentiable\footnote{\url{https://en.wikipedia.org/wiki/Weak_derivative}} as a function of $\mathbf{Y}$.
Before that, we can ask ourselves if this function is even continuous: if
we change $\mathbf{Y}$, the active set of the solution $\mathbf{B}$ can change/jump thus creating discontinuities.
Let the support of the solution be $A = \text{supp}(\hat{\mathbf{B}})$. Here, \textbf{we mean that $A$ is the set of activated rows}.

\newpage

We can express the LASSO fit as follows:

\begin{equation*}
    \mathbf{XB} = \mathbf{X}_A
    (\mathbf{X}_A^{\top}\mathbf{X}_A)^{-1}
    (\mathbf{B}^{\top}_A\mathbf{Y} - \mathbf{\Lambda}\mathbf{S}_A)
    \enspace .
\end{equation*}
%
where $\mathbf{X}_A$ (respectively $\mathbf{B}_A$) indexes the columns of $\bf X$ (respectively $\mathbf{B}$) in $A$. Furthermore, $\mathbf{S}_A$ are the signs of the active
LASSO coefficients and $\bf\Lambda$ is a matrix full of lambda. To prove that this function is continuous, we need to use the dual of the LASSO, which reads:

\begin{align*}
    &\min_{\mathbf{\Theta} \in \bbR^{n \times T}} \norm{ \mathbf{\Theta} - \mathbf{Y} }_{\text{F}}^2 \\
    &\text{s.t.} \quad \norm{\mathbf{X^{\top}\Theta}}_{2, \infty} \leq \lambda
    \enspace .
\end{align*}
%
and the relationship between the dual $\hat{\bf{\Theta}}$ and the primal $\hat{\bf{B}}$ solutions is:

\begin{equation*}
    \bf{X} \hat{\bf{B}} = \mathbf{Y - \hat{\Theta}}
    \enspace .
\end{equation*}
%
Yet the dual solution $\hat{\bf{\Theta}}$ is the projection of $\mathbf{Y}$ onto the convex polyhedron $C = \{\bf{\Theta}, \norm{\mathbf{X^{\top}\Theta}}_{2, \infty} \leq \lambda\}$,
which we denote $\hat{\bf{\Theta}} = P_C(\bf{Y})$. Therefore, the \nameref{section_4} fit is simply:

\begin{equation*}
    \bf{X} \hat{\bf{B}} = (I - P_C)(\mathbf{Y})
    \enspace .
\end{equation*}
%
which is the residual of the projection of $\bfY$ onto $C$.
\\
\\
Yet, we know that a projection onto a convex set is a non-expansive mapping, the same is true for the residual map. It follows:

\begin{equation*}
    \forall \mathbf{Y}, \mathbf{Y'}, \norm{(I - P_C(\mathbf{Y})) - (I - P_C(\mathbf{Y'}))}_{\text{F}} \leq \norm{\bf{Y - Y'}}_{\text{F}}
    \enspace .
\end{equation*}
%
From the previous equation, we deduce that the LASSO fit is a Lipschitz-continuous mapping, which by Rademacher's theorem, implies that it is almost differentiable everywhere.
\\
\\
The continuity can be interpreted as follows: when $\bfY$ changes as well as the active set $A$, \textbf{the coefficients of variables to leave
the active set continuously drop to zero, and coefficients of variables to enter the active set continuously move from zero}. This makes the LASSO continuous.

\subsection*{Finding the closed-form expression of SURE for \nameref{section_4}}

Now, we want to derive a closed-form expression of SURE for \nameref{section_4}. Let's re-examine the LASSO fit column-wise (\textit{i.e.} task by task), for $t=1, \dots, T$:
%
\begin{equation*}
    \mathbf{X B}_{:t}
    = \mathbf{X}_A(\mathbf{X}_A^{\top}\mathbf{X}_A)^{-1}
    (\mathbf{B}^{\top}_{A, :t}\mathbf{Y}_{:t} - \mathbf{\Lambda}\mathbf{S}_{A, t})
    \enspace .
\end{equation*}
%
We take the reasonable assumption that $A$ and $\mathbf{S}_A$ are locally constant, meaning that a small change in $\mathbf{Y}$ won't change the active set nor the signs of the active
coefficients. Remember that finding a closed-form expression of SURE boils down to computing the divergence of the LASSO estimator.
The degrees of freedom of the LASSO writes:
%
\begin{align*}
    \text{df}(\mathbf{XB})
    &= \sum_{i=1}^n \sum_{t=1}^T \frac{\partial (\mathbf{XB})_{i,t}}{\partial (\mathbf{Y})_{i, t}} \\
    &= \sum_{t=1}^T \left ( \sum_{i=1}^n \frac{\partial (\mathbf{XB})_{i,t}}{\partial (\mathbf{Y})_{i, t}} \right ) \\
    &= \sum_{t=1}^T \text{tr}(\mathbf{X}_A (\mathbf{X}_A^{\top} \mathbf{X}_A)^{-1}\mathbf{X}_A^{\top}) \\
    &= \sum_{t=1}^T \text{tr}(\mathbf{X}_A^{\top}\mathbf{X}_A (\mathbf{X}_A^{\top} \mathbf{X}_A)^{-1}) \\
    &= T \lvert A \rvert
\end{align*}
%
where $\lvert A \rvert$ is the cardinal of the active set. This yields the following closed-form expression for the risk estimate:
%
\begin{equation*}
    \text{SURE}(\hat{\bf{B}}) =
    - n T \sigma^2
    + \norm{\mathbf{Y} - \mathbf{X \hat{B}}}^2_{\text{F}}
    + 2 \sigma^2 T \lvert A \rvert
    \enspace .
\end{equation*}
%
This seems a very reasonable answer since the number of degrees of freedom (number of parameters in a model allowed to change) in LASSO is the active set, meaning the set of non-zero
coefficients.

\subsection*{The technicalities of SURE for reweighted Multi-Task LASSO}

The previous reasoning can't be trivially applied to the reweighted case. Indeed, the non-convex penalty seen at \nameref{section_5} can prevent
the derivation of a closed-form solution or be very challenging. It needs to be treated on a case-by-case basis. In the following we review two
techniques to evaluate numerically SURE. A more comprehensive review was carried out by \cite{Deledalle_Vaiter_Fadili_Peyre14}. Our goal here is
to find a numerical way to efficiently compute $\text{df}(\bf\hat{B})$. Evaluating such a term costs $O(Tn^2)$.
\\
\\
Note that $\sigma$ is an unknown quantity, thus it needs to be estimated with an additional parameter.
%
\subsubsection*{Monte-Carlo SURE}

The Monte-Carlo sampling technique consists in evaluating the degrees of freedom by randomly sampling $p$ vectors that will serve to compute $p$ directional derivatives
of $\mathbf{Y} \rightarrow \mathbf{\hat{B}(Y)}$, where $\hat{\bf B}$ is some estimator of $\bf B_0$ obtained using a non-convex penalty.
%
More formally, for $t = 1, \dots, T$, \cite{Ramani_Blu_Unser08} infers that:

\begin{equation*}
    \text{df}(\mathbf{\hat{B}}) =
    \sum_{t=1}^T \bbE_{\Delta_t \sim \mathcal{N}(0, \text{Id}_\text{n})}
    \left \langle
            \frac{\partial \mathbf{\hat{B}}_{:t} }{
            \partial \mathbf{Y}_{:t}} \Delta_t, \mathbf{X X}^{\top} \Delta_t
    \right \rangle
    \enspace .
\end{equation*}
%
We observe numerically that for the recovery problems we consider, it provides a very accurate estimator of the trace, while
it only costs in the multi-task case $O(Tn)$. Now, we need to study a more tedious question: how to reliably and efficiently
evaluate the directional derivative for $t=1, \dots, T$, $\frac{\partial \mathbf{\hat{B}}_{:t} }{\mathbf{Y}_{:t}}$.

\subsubsection*{Finite-difference SURE}

The most straightforward way to evaluate such a partial derivative is using finite difference methods.
Since the notation we have used so far is cumbersome, we will adopt a lighter one. Let $\hat{\bfB}(\bfY) \in \bbR^{p\times T}$ be an estimator
of $\mathbf{B}_0 \in \mathbb{R}^{p\times T}$ which depends on $\mathbf{Y} \in \bbR^{n \times T}$ (\textit{e.g.} a \nameref{section_5} fit for which the divergence
can't be evaluate in closed-form). Let $(e_i)_{1 \leq i \leq n}$ be the canonical basis of $\bbR^n$. The finite difference expression for computing
the degrees of freedom of a multi-task regression fit writes:

\begin{equation*}
    \forall \epsilon > 0, \enspace \text{df}(\mathbf{\hat{B}}) \approx
    \frac{1}{\epsilon} \sum_{t=1}^T \sum_{i=1}^n
        \left(
            \mathbf{\hat{B}}_{:t}(\mathbf{Y}_{:t} + \epsilon e_i)
            - \mathbf{\hat{B}}_{:t}(\mathbf{Y}_{:t})
        \right)
    \enspace .
\end{equation*}
%
The main advantage of this method is that $\mathbf{Y} \rightarrow \mathbf{\hat{B}(Y)}$ can be used as a black-box without knowing the underlying algorithm that provides
the fit. This generic method allows us to circumvent the hurdle of evaluating a closed-form expression for each penalty in the case of \nameref{section_5}.
\cite{Deledalle_Vaiter_Fadili_Peyre14} observes that in practice this method yields quasi unbiased risk estimator (\textit{i.e.} with neglectable bias). They
found a heuristic for $\epsilon = 2 \sigma / n^{0.3}$.

\subsection*{Combining Monte-Carlo and finite difference}

The finite difference method still requires to evaluate $O(nT)$ finite differences, which can be significant. In that case the
Monte-Carlo approach can be used in conjunction with finite difference methods, to speed up the whole process. Then, for $\epsilon > 0$,
the Finite Difference Monte Carlo (FDMC) writes:

\begin{equation*}
    \text{df}(\hat{\mathbf{B}}) \approx
\end{equation*}


\newpage
\bibliographystyle{plainnat}
\bibliography{references_all}



\end{document}
