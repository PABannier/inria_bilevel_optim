\documentclass[a4paper,10pt]{article}
\usepackage[margin=2.5cm]{geometry}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{bm}




\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cross-referencing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithms
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[titlenumbered,ruled,noend,algo2e]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetEndCharOfAlgoLine{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{fancyvrb}                  % for fancy verbatim
\usepackage{textcomp}
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}
\usepackage[procnames]{listings}
% \usepackage{setspace} % need for \setstretch{1}
\lstset{%
language   = python,%
 % basicstyle = \ttfamily\setstretch{1},%
basicstyle = \ttfamily,%
columns    = flexible,%
keywordstyle=\color{javared},
firstnumber=100,
frame=shadowbox,
showstringspaces=false,
morekeywords={import,from,class,def,for,while,if,is,in,elif,
else,not,and,or,print,break,continue,return,True,False,None,access,
as,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert,!=},
keywordstyle={\color{javared}\bfseries},
commentstyle=\color{javagreen}, %vsomb_col white comments
morecomment=[s][\color{javagreen}]{"""}{"""},
upquote=true,
%% style for number
numbers=none,
resetmargins=true,
xleftmargin=10pt,
linewidth= \linewidth,
numberstyle=\tiny,
stepnumber=1,
numbersep=8pt, %
frame=shadowbox,
rulesepcolor=\color{black},
procnamekeys={def,class},
procnamestyle=\color{oneblue}\textbf,
literate={á}{{\'a}}1
{à}{{\`a }}1
{ã}{{\~a}}1
{é}{{\'e}}1
{ê}{{\^e}}1
{è}{{\`e}}1
{í}{{\'i}}1
{î}{{\^i}}1
{ó}{{\'o}}1
{õ}{{\~o}}1
{ô}{{\^o}}1
{ú}{{\'u}}1
{ü}{{\"u}}1
{ç}{{\c{c}}}1
}


\usepackage{times} % use Times

\usepackage{shortcuts_js} % possibly adapted from https://github.com/josephsalmon/OrganizationFiles/sty/shortcuts_js.sty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use prebuiltimages/ for images extracted from code (e.g. python)
% or to share images built from a software not available by the whole team (say matlab .fig, or inskcape .svg).
% .svg files should be stored in dir srcimages/ and built from moosetex if needed:
% https://www.charles-deledalle.fr/pages/moosetex.php
% NEVER (GIT) versions files in images/ : only prebuiltimages/ & srcimages/ !

\usepackage{graphicx} % For figures
\graphicspath{{images/}, {prebuiltimages/}}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[authoryear]{natbib}
\usepackage{cleveref} % mandatory for no pbs with hyperlinks theorem etc\dots
\crefformat{equation}{Eq.~(#2#1#3)} % format for equations
\Crefformat{equation}{Equation~(#2#1#3)} % format for equations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header and document start
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{Pierre-Antoine Bannier}
\title{A quick guided tour of LASSO-like models}

\begin{document}

\maketitle

\vskip 0.3in

In this memo, $\mathbf{\Phi} \in \bbR^{n \times p}$ is the design matrix, $x \in \bbR^{p}$ (resp. $\mathbf{X} \in \bbR^{p \times T}$) is the coefficient vector (resp. matrix) and $y \in \bbR^n$
(resp. $\mathbf{Y} \in \bbR^{n \times T}$) is the target vector (resp. matrix). Let $n, p, T \in \mathbb{N}^*$ be respectively the number of samples, the number of features and the number of tasks. As we shall see later, LASSO-like models
are particularly useful for large-dimensional problems \textit{i.e.} when $p \gg n$. $\lambda \in \bbR^+$ is a regularizing hyperparameter.

\section{LASSO}
\label{section_1}

In this section, we assume that $T = 1$ and $p \gg n$.

\vskip 0.2in

LASSO consists in solving the following optimization problem:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^{p}} \frac{1}{2n}\norm{ y - \mathbf{\Phi} x}_2^2 + \lambda \norm{ x}_{1}
\end{equation*}

\vskip 0.1in

LASSO is a useful estimator to reconstruct sparse signals. It acts as a convex regularizer surrogate for the $l_0$ norm. Indeed, the
$l_0$ norm yields a $NP$-hard combinatorial non-convex problem to solve. \\

\subsection*{Why do we want sparsity?}

For large-dimensional problems, there exists an infinite set of solutions (overcomplete set).
Imposing a structure constraint on the solution $x$ is critical to find a unique solution to the problem.

\subsection*{How is this problem solved?}

Unlike unregularized OLS problem, the LASSO estimator does not generally have a closed form solution. Therefore, we need to rely on optimization methods for solving this optimization problem. Actually, this convex problem
can be recast as a linear pogram. In practice, \textbf{proximal coordinate descent} has proved to be the most efficient method for solving such problems, see \cite{Bertrand_Massias_Anderson}.

\section{Group LASSO}
\label{section_2}

There are many regression problems where the features have a natural group structure, and it is desirable to have all coefficients within a group become nonzero or zero simulatenously.
\\
Consider a partitioning of the features such that we can partition the columns of $\mathbf{\Phi}$ as $\{\phi_{1}, \dots, \phi_{J}\}$ where $\phi_{j} \in \bbR^{n \times p_j}$ and
$p_j$ is the number of features in group $j$. We can apply the same partition to the coefficients of the coefficient vector $x \in \bbR^p$. Indeed, $x$ is of the form $(x_{1}, \dots, x_{j})$ where
$x_{j} \in \bbR^{p_j}$. Note that we have as many unique coefficients in $x$ as there are groups in the partition of the features in the design matrix.
\\

Then, the optimization problem reads:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \Biggl\{ \frac{1}{2n} \sum_{i=1}^n (y_i - \sum_{j=1}^J \phi_{j}^\top x_{j})^2 + \lambda \sum_{j=1}^{J} \norm{ x_{j}}_2 \Biggr\}
\end{equation*}

$ \norm{x_{j}}$ is the Euclidean norm of the vector $x_{j}$. We can convince ourselves that if $\forall j \in \{1, \dots, J\}, p_j = 1$, then $\norm{ x_{j}}_2 = \lvert x_{j} \rvert$,
thus the problem reduces to \nameref{section_1}.

\subsection*{Why is this $l_1 / l_2$-mixed penalty still inducing sparsity?}

The penalty set on the optimization problem is called a $l_1/l_2$-mixed norm.
\\

First, we are computing the $l_2$ norm of the coefficients in one group, and then we are summing all of them. Simply put, the $l_1/l_2$-mixed norm can be seen
as the $l_1$ norm of the $l_2$ norms, which means that there will be few non-zero $l_2$ norms, which in turn creates group sparsity. Indeed, remember that since $\lvert \lvert \cdot \rvert \rvert_2$ is a norm, for any vector $x$,
$\norm{ x}_2 = 0 \Rightarrow x = 0$.

\subsection*{How is this optimization problem solved?}

This problem is again solved using proximal coordinate descent. However, we are no longer iterating over features one by one but group by group, leading to the \textbf{proximal block coordinate descent}.

\section{Adaptive/Weighted LASSO}
\label{section_3}

Recall that the $l_1$ norm is a convex surrogate of the $l_0$ norm. However, it has some limitations compared to
the $l_0$-norm. Indeed, the $l_1$-norm is undemocratic in that it penalizes more heavily coefficients larger coefficients than smaller ones. It turns out that the LASSO penalty exerts less pressure on small coefficients,
thus letting them being small while not setting them to zero. In this vein, the adaptive LASSO has been proposed as a way for fitting models sparser than \nameref{section_1}, by introducing weights in front of every coefficient that is
penalized. The optimization problem reads:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \Biggl\{ \frac{1}{2n} \norm{ y - \mathbf{\Phi}x}_2^2 + \lambda \sum_{j=1}^p w_j \lvert x_j \rvert \Biggr\}
\end{equation*}
\\
where $w \in \bbR^p$ is a weight vector. By placing $w \in \bbR^p$ on the diagonal of a square matrix $\mathbf{W} \in \bbR^{p \times p}$, we can rewrite the optimization problem in a matrix form:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \frac{1}{2n} \norm{y - \mathbf{\Phi}x}_2^2 + \lambda \norm{\mathbf{W}x}_1
\end{equation*}

\subsection*{How to choose the weight vector?}

The rationale is that large weights could be used to discourage nonzero entries in the recovered signal, while small weights would encourage them. As a rule of thumb, the weights should relate inversely
to the true signal magnitude (in the context of signal processing, which is of interest here). However, we don't have access to the true signal magnitudes. The question remains how a valid set of weights
may be obtained without first knowing the true signal. \cite{Candes_Wakin_Boyd08} argues that there may exist a range of favorable weighting matrices $\mathbf{W} \in \bbR^{p \times p}$.
\\
\\
They propose a majorization-minimization (MM) algoritm to iteratively reweight the coefficients of the LASSO estimator.
Recall that a MM algorithm works by iteratively minimizing a surrogate function that majorizes the objective function.
Simply put, first we majorize the objective function by a surrogate, then we minimize this surrogate function \footnote{It is a core idea in optimization, used to derive gradient descent for instance.
For a convex $L$-smooth function, we majorize it by its quadratic upper bound, then we minimize this quadratic upper bound.
The quadratic upper bound is easily minimized since it has a closed-form solution.
For more details, see \url{http://josephsalmon.eu/enseignement/UW/STAT593/MajorizationMinimization.pdf}}.
\\
Indeed, consider the following constrained problem:

\begin{equation*}
    \min_{x \in \bbR^p} \quad  \sum_{i=1}^p \log(\lvert x_i \rvert + \epsilon) \quad \textrm{s.t.} \quad  y = \mathbf{\Phi} x
\end{equation*}
\\

Clearly this problem is equivalent to the following:
\begin{equation*}
    \min_{x,u \in \bbR^p} \quad  \sum_{i=1}^p \log(u_i + \epsilon) \quad \textrm{s.t.} \quad  y = \mathbf{\Phi} x, \quad \lvert x_i \rvert \leq u_i, \enspace i=1,\dots,p
\end{equation*}

This problem is easier to solve since we get rid of the absolute value in the minimized term as it is now placed as an additional constraint.
\\
To apply a MM algorithm, we need to majorize the log-sum function. Yet, the log-sum function is concave and therefore below its tangent. Thus by taking a first-order Taylor expansion, we obtain a linearization of
the log-sum function in a neighborhood of $u$.
\\
More formally, let $g(u) = \sum_{i=1}^p \log(u_i + \epsilon)$. The first-order Taylor expansion in a neighborhood of $u^{(l)} \in \bbR^p$ yields:

\begin{align*}
    u^{(l+1)} &= \argmin_{u \in \bbR^p} g(u^{(l)}) + \nabla g(u^{(l)})\cdot (u - u^{(l)}) \\
              &= \argmin_{u \in \bbR^p} \sum_{i=1}^p \log( u_i^{(l)} + \epsilon) + (u - u^{(l)}) \sum_{i=1}^p \frac{1}{ u_i^{(l)}  + \epsilon}
\end{align*}

By removing the terms that do not depend on $u$, it follows that:

\begin{equation*}
    u^{(l+1)} = \argmin_{u \in \bbR^p} \sum_{i=1}^p \frac{u_i}{ u_i^{(l)} + \epsilon}
\end{equation*}

And by equivalence,

\begin{equation*}
    x^{(l+1)} = \argmin_{x \in \bbR^p} \sum_{i=1}^p \frac{\lvert x_i \rvert}{\lvert x_i^{(l)} \rvert + \epsilon}
\end{equation*}

By letting $\forall i \in \{1, \dots, p\}, w_i^{(l)} = \frac{1}{\lvert x_i^{(l)}\rvert + \epsilon}$, it follows:

\begin{equation*}
    x^{(l+1)} = \argmin_{x \in \bbR^p} \sum_{i=1}^p w_i^{(l)} \lvert x_i \rvert
\end{equation*}
\\
Therefore, it follows an algorithm to iteratively reweight the coefficients of the LASSO model.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative reweighted l1 minimization}
}
%
\Input{$
    \mathbf{\Phi} \in \bbR^{n \times p},
    x \in \bbR^{p},
    y \in \bbR^{n},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+,
    \lambda \in \bbR^+
    $}

\Init{$w = \ind_{\bbR^p} $}

    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $x^{(l)} \leftarrow \argmin \norm{ y -  \mathbf{\Phi} x}_2^2 + \lambda \norm{ w \cdot x}_1$

        Update the weights: for each
        $i = 1, \dots, p, \enspace w_i^{(l+1)} \leftarrow \frac{1}{\lvert x_i^{(l)} \rvert + \epsilon}$
    }

\Return{$x$}
\end{algorithm}
}
\jo{the math seems wrong in the algorithm above. double check it!}

\vskip 0.2in

Note that this algorithm minimizes a concave objective (log-sum is concave), by iteratively solving convex subproblems. Hence, due to the concavity of the objective function, we
are not guaranteed to converge to a global minimum.

\subsection*{Why choosing the log-sum function? How to choose $\epsilon$?}

The log-sum penalty function has the potential to be more sparsity-encouraging than the $l_1$ norm (for a visual explanation, see \cite{Candes_Wakin_Boyd08}). We clearly see
that the log-sum penalty is closer from the $l_0$ norm than the $l_1$ norm thus encouraging smaller coefficients to be set to zero.
\\
More precisely, the smaller $\epsilon$ the closer the log-sum function is from the $l_0$ norm. However, $\epsilon$ can't be set arbitrarily small since as $\epsilon \rightarrow 0$ it
becomes more likely that the iterative reweighted $l_1$ algorithm will be stuck at a local optimum. In practice, note that signal reconstruction is robust to the choice of $\epsilon$.

\section{Multi-Task LASSO}
\label{section_4}

In this section, we assume that $T > 1$. Therefore, we are now predicting a multivariate response $\mathbf{Y}\in \bbR^{n \times T}$, and by assuming that the data collected are linear measurements, the model reads:

\begin{equation*}
    \mathbf{Y} = \mathbf{\Phi}\mathbf{X} + \mathbf{E}
\end{equation*}
\\
where $\mathbf{E} \in \bbR^{n \times T}$ is a matrix of errors (noise), usually simulated with a pre-defined signal-to-noise ratio, when doing compressive sensing. Note that $\mathbf{X} \in \bbR^{p \times T}$ is a matrix.
Like in \nameref{section_2}, suppose that there exists an unknown subset $S \subset  \{1, \dots, p\}$ of the features that are relevant for prediction, and that \textbf{this same subset is preserved across all K components} of the response
variable. In this case, it is natural to consider a group LASSO penalty, in which the $p$ groups are defined by the rows $\{x_{j} \in \bbR^T, j=1,\dots,p\}$ of $\mathbf{X}$.
\\
Therefore, the optimization problem we need to solve is:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \Biggl( \sum_{j=1}^p \norm{ x_{j}}_2 \Biggr)
\end{equation*}
\\
Using the $l_1/l_2$ mixed-norm, it reads:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \norm{ \mathbf{X}}_{2, 1}
\end{equation*}
\\
Note that the fitting term is now a matrix norm (here the Frobenius norm) since we are dealing with matrices. As stated in \nameref{section_2}, we are using a mixed $l_1/l_2$ norm to enforce group sparsity on the coefficients $\mathbf{X}$. Indeed,
we are first computing the $l_2$ norm of every line of the coefficient matrix, then we are computing the $l_1$ norm of the remaining vectors full of the $l_2$ norm of the line of the coefficient matrix.

\subsection*{How is this optimization problem solved?}

In scikit-learn, the algorithm used to fit the model is coordinate descent.

\section{Multi-Task Reweighted LASSO}
\label{section_5}

The Multi-Task Reweighted LASSO is a combination of \nameref{section_3} and \nameref{section_4}. The goal of the model is to predict a multivariate response variable while enforcing a sparser solution than \nameref{section_1}.
\\
To induce a group-sparsity penalty like in \nameref{section_2}, we compute the $l_2$ norm of the lines of the coefficient matrix $\mathbf{X} \in \bbR^{p \times T}$, then we apply like in \nameref{section_3} a weighting matrix
on the resulting vector.
\\
Therefore, the optimization problems reads:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \sum_{j=1}^p \underbrace{g(\norm{\mathbf{X}_{j:}}_{2})}_{g(x) \leq g(x^{(k)}) + \langle \nabla g (x^{(k)}) , x - x^{(k)} \rangle }
\end{equation*}

where $g$ is any penalty convex or non-convex. In \cite{Candes_Wakin_Boyd08}, $g(x) = \sum_{i=1}^p \log(\lvert x_i \rvert + \epsilon)$. Algorithm 2 shows the log-concave reweighted $l_1$ minimization.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative Multi-task reweighted l1 minimization}
}
%
\Input{$
    \mathbf{\Phi} \in \bbR^{n \times p},
    \mathbf{X} \in \bbR^{p\times T},
    \mathbf{Y} \in \bbR^{n\times T},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+_*,
    \lambda \in \bbR^+
    $}
    \Init{$w = \ind_{\bbR^p} $}

    % Fill  with ones \\
    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $\mathbf{X} \leftarrow \argmin \norm{\mathbf{Y} -  \mathbf{\Phi} \mathbf{X}}_{\text{F}}^2 + \lambda \norm{\text{diag}(w) \cdot \mathbf{X}}_{2, 1}$

        Update the weights: for each
        $j = 1, \dots, p, \enspace w_j \leftarrow \frac{1}{\norm{\mathbf{X}_{j:}}_2 + \epsilon}$
    }

\Return{$\mathbf{X}$}
\end{algorithm}
}

\newpage
\bibliographystyle{plainnat}
\bibliography{references_all}



\end{document}
