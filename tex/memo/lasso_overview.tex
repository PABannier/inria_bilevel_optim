\documentclass[a4paper,10pt]{article}
\usepackage[margin=2.5cm]{geometry}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{color}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage{bm}




\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cross-referencing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Algorithms
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[titlenumbered,ruled,noend,algo2e]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetEndCharOfAlgoLine{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{fancyvrb}                  % for fancy verbatim
\usepackage{textcomp}
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}
\usepackage[procnames]{listings}
% \usepackage{setspace} % need for \setstretch{1}
\lstset{%
language   = python,%
 % basicstyle = \ttfamily\setstretch{1},%
basicstyle = \ttfamily,%
columns    = flexible,%
keywordstyle=\color{javared},
firstnumber=100,
frame=shadowbox,
showstringspaces=false,
morekeywords={import,from,class,def,for,while,if,is,in,elif,
else,not,and,or,print,break,continue,return,True,False,None,access,
as,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert,!=},
keywordstyle={\color{javared}\bfseries},
commentstyle=\color{javagreen}, %vsomb_col white comments
morecomment=[s][\color{javagreen}]{"""}{"""},
upquote=true,
%% style for number
numbers=none,
resetmargins=true,
xleftmargin=10pt,
linewidth= \linewidth,
numberstyle=\tiny,
stepnumber=1,
numbersep=8pt, %
frame=shadowbox,
rulesepcolor=\color{black},
procnamekeys={def,class},
procnamestyle=\color{oneblue}\textbf,
literate={á}{{\'a}}1
{à}{{\`a }}1
{ã}{{\~a}}1
{é}{{\'e}}1
{ê}{{\^e}}1
{è}{{\`e}}1
{í}{{\'i}}1
{î}{{\^i}}1
{ó}{{\'o}}1
{õ}{{\~o}}1
{ô}{{\^o}}1
{ú}{{\'u}}1
{ü}{{\"u}}1
{ç}{{\c{c}}}1
}


\usepackage{times} % use Times

\usepackage{shortcuts_js} % possibly adapted from https://github.com/josephsalmon/OrganizationFiles/sty/shortcuts_js.sty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use prebuiltimages/ for images extracted from code (e.g. python)
% or to share images built from a software not available by the whole team (say matlab .fig, or inskcape .svg).
% .svg files should be stored in dir srcimages/ and built from moosetex if needed:
% https://www.charles-deledalle.fr/pages/moosetex.php
% NEVER (GIT) versions files in images/ : only prebuiltimages/ & srcimages/ !

\usepackage{graphicx} % For figures
\graphicspath{{images/}, {prebuiltimages/}}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[authoryear]{natbib}
\usepackage{cleveref} % mandatory for no pbs with hyperlinks theorem etc\dots
\crefformat{equation}{Eq.~(#2#1#3)} % format for equations
\Crefformat{equation}{Equation~(#2#1#3)} % format for equations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header and document start
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{Pierre-Antoine Bannier}
\title{A quick guided tour of LASSO-like models and bi-level optimization}

\begin{document}

\maketitle

\vskip 0.3in

In this memo, $\mathbf{\Phi} \in \bbR^{n \times p}$ is the design matrix, $x \in \bbR^{p}$ (resp. $\mathbf{X} \in \bbR^{p \times T}$) is the coefficient vector (resp. matrix) and $y \in \bbR^n$
(resp. $\mathbf{Y} \in \bbR^{n \times T}$) is the target vector (resp. matrix). Let $n, p, T \in \mathbb{N}^*$ be respectively the number of samples, the number of features and the number of tasks. As we shall see later, LASSO-like models
are particularly useful for large-dimensional problems \textit{i.e.} when $p \gg n$. $\lambda \in \bbR^+$ is a regularizing hyperparameter. We write $\bbS^n$ for the set of symmetric matrices and $\bbS^n_+$ (respectively $\bbS^n_{++}$) for
the set of positive semi-definite (respectively positive definite) matrices.

\section{LASSO}
\label{section_1}

In this section, we assume that $T = 1$ and $p \gg n$.

\vskip 0.2in

LASSO consists in solving the following optimization problem:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^{p}} \frac{1}{2n}\norm{ y - \mathbf{\Phi} x}_2^2 + \lambda \norm{ x}_{1}
\end{equation*}

\vskip 0.1in

LASSO is a useful estimator to reconstruct sparse signals. It acts as a convex regularizer surrogate for the $l_0$ norm. Indeed, the
$l_0$ norm yields a $NP$-hard combinatorial non-convex problem to solve. \\

\subsection*{Why do we want sparsity?}

For large-dimensional problems, there exists an infinite set of solutions (overcomplete set).
Imposing a structure constraint on the solution $x$ is critical to find a unique solution to the problem.

\subsection*{How is this problem solved?}

Unlike unregularized OLS problem, the LASSO estimator does not generally have a closed form solution. Therefore, we need to rely on optimization methods for solving this optimization problem. Actually, this convex problem
can be recast as a linear pogram. In practice, \textbf{proximal coordinate descent} has proved to be the most efficient method for solving such problems, see \cite{Bertrand_Massias_Anderson}.

\section{Group LASSO}
\label{section_2}

There are many regression problems where the features have a natural group structure, and it is desirable to have all coefficients within a group become nonzero or zero simulatenously.
\\
Consider a partitioning of the features such that we can partition the columns of $\mathbf{\Phi}$ as $\{\phi_{1}, \dots, \phi_{J}\}$ where $\phi_{j} \in \bbR^{n \times p_j}$ and
$p_j$ is the number of features in group $j$. We can apply the same partition to the coefficients of the coefficient vector $x \in \bbR^p$. Indeed, $x$ is of the form $(x_{1}, \dots, x_{j})$ where
$x_{j} \in \bbR^{p_j}$. Note that we have as many unique coefficients in $x$ as there are groups in the partition of the features in the design matrix.
\\

Then, the optimization problem reads:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \Biggl\{ \frac{1}{2n} \sum_{i=1}^n (y_i - \sum_{j=1}^J \phi_{j}^\top x_{j})^2 + \lambda \sum_{j=1}^{J} \norm{ x_{j}}_2 \Biggr\}
\end{equation*}

$ \norm{x_{j}}$ is the Euclidean norm of the vector $x_{j}$. We can convince ourselves that if $\forall j \in \{1, \dots, J\}, p_j = 1$, then $\norm{ x_{j}}_2 = \lvert x_{j} \rvert$,
thus the problem reduces to \nameref{section_1}.

\subsection*{Why is this $l_1 / l_2$-mixed penalty still inducing sparsity?}

The penalty set on the optimization problem is called a $l_1/l_2$-mixed norm.
\\

First, we are computing the $l_2$ norm of the coefficients in one group, and then we are summing all of them. Simply put, the $l_1/l_2$-mixed norm can be seen
as the $l_1$ norm of the $l_2$ norms, which means that there will be few non-zero $l_2$ norms, which in turn creates group sparsity. Indeed, remember that since $\lvert \lvert \cdot \rvert \rvert_2$ is a norm, for any vector $x$,
$\norm{ x}_2 = 0 \Rightarrow x = 0$.

\subsection*{How is this optimization problem solved?}

This problem is again solved using proximal coordinate descent. However, we are no longer iterating over features one by one but group by group, leading to the \textbf{proximal block coordinate descent}.

\section{Adaptive/Weighted LASSO}
\label{section_3}

Recall that the $l_1$ norm is a convex surrogate of the $l_0$ norm. However, it has some limitations compared to
the $l_0$-norm. Indeed, the $l_1$-norm is undemocratic in that it penalizes more heavily coefficients larger coefficients than smaller ones. It turns out that the LASSO penalty exerts less pressure on small coefficients,
thus letting them being small while not setting them to zero. In this vein, the adaptive LASSO has been proposed as a way for fitting models sparser than \nameref{section_1}, by introducing weights in front of every coefficient that is
penalized. The optimization problem reads:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \Biggl\{ \frac{1}{2n} \norm{ y - \mathbf{\Phi}x}_2^2 + \lambda \sum_{j=1}^p w_j \lvert x_j \rvert \Biggr\}
\end{equation*}
\\
where $w \in \bbR^p$ is a weight vector. By placing $w \in \bbR^p$ on the diagonal of a square matrix $\mathbf{W} \in \bbR^{p \times p}$, we can rewrite the optimization problem in a matrix form:

\begin{equation*}
    x^* = \argmin_{x \in \bbR^p} \frac{1}{2n} \norm{y - \mathbf{\Phi}x}_2^2 + \lambda \norm{\mathbf{W}x}_1
\end{equation*}

\subsection*{How to choose the weight vector?}

The rationale is that large weights could be used to discourage nonzero entries in the recovered signal, while small weights would encourage them. As a rule of thumb, the weights should relate inversely
to the true signal magnitude (in the context of signal processing, which is of interest here). However, we don't have access to the true signal magnitudes. The question remains how a valid set of weights
may be obtained without first knowing the true signal. \cite{Candes_Wakin_Boyd08} argues that there may exist a range of favorable weighting matrices $\mathbf{W} \in \bbR^{p \times p}$.
\\
\\
They propose a majorization-minimization (MM) algoritm to iteratively reweight the coefficients of the LASSO estimator.
Recall that a MM algorithm works by iteratively minimizing a surrogate function that majorizes the objective function.
Simply put, first we majorize the objective function by a surrogate, then we minimize this surrogate function \footnote{It is a core idea in optimization, used to derive gradient descent for instance.
For a convex $L$-smooth function, we majorize it by its quadratic upper bound, then we minimize this quadratic upper bound.
The quadratic upper bound is easily minimized since it has a closed-form solution.
For more details, see \url{http://josephsalmon.eu/enseignement/UW/STAT593/MajorizationMinimization.pdf}}.
\\
Indeed, consider the following constrained problem:

\begin{equation*}
    \min_{x \in \bbR^p} \quad  \sum_{i=1}^p \log(\lvert x_i \rvert + \epsilon) \quad \textrm{s.t.} \quad  y = \mathbf{\Phi} x
\end{equation*}
\\

Clearly this problem is equivalent to the following:
\begin{equation*}
    \min_{x,u \in \bbR^p} \quad  \sum_{i=1}^p \log(u_i + \epsilon) \quad \textrm{s.t.} \quad  y = \mathbf{\Phi} x, \quad \lvert x_i \rvert \leq u_i, \enspace i=1,\dots,p
\end{equation*}

This problem is easier to solve since we get rid of the absolute value in the minimized term as it is now placed as an additional constraint.
\\
To apply a MM algorithm, we need to majorize the log-sum function. Yet, the log-sum function is concave and therefore below its tangent. Thus by taking a first-order Taylor expansion, we obtain a linearization of
the log-sum function in a neighborhood of $u$.
\\
More formally, let $g(u) = \sum_{i=1}^p \log(u_i + \epsilon)$. The first-order Taylor expansion in a neighborhood of $u^{(l)} \in \bbR^p$ yields:

\begin{align*}
    u^{(l+1)} &= \argmin_{u \in \bbR^p} g(u^{(l)}) + \nabla g(u^{(l)})\cdot (u - u^{(l)}) \\
              &= \argmin_{u \in \bbR^p} \sum_{i=1}^p \log( u_i^{(l)} + \epsilon) + (u - u^{(l)}) \sum_{i=1}^p \frac{1}{ u_i^{(l)}  + \epsilon}
\end{align*}

By removing the terms that do not depend on $u$, it follows that:

\begin{equation*}
    u^{(l+1)} = \argmin_{u \in \bbR^p} \sum_{i=1}^p \frac{u_i}{ u_i^{(l)} + \epsilon}
\end{equation*}

And by equivalence,

\begin{equation*}
    x^{(l+1)} = \argmin_{x \in \bbR^p} \sum_{i=1}^p \frac{\lvert x_i \rvert}{\lvert x_i^{(l)} \rvert + \epsilon}
\end{equation*}

By letting $\forall i \in \{1, \dots, p\}, w_i^{(l)} = \frac{1}{\lvert x_i^{(l)}\rvert + \epsilon}$, it follows:

\begin{equation*}
    x^{(l+1)} = \argmin_{x \in \bbR^p} \sum_{i=1}^p w_i^{(l)} \lvert x_i \rvert
\end{equation*}
\\
Therefore, it follows an algorithm to iteratively reweight the coefficients of the LASSO model.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative reweighted l1 minimization}
}
%
\Input{$
    \mathbf{\Phi} \in \bbR^{n \times p},
    x \in \bbR^{p},
    y \in \bbR^{n},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+,
    \lambda \in \bbR^+
    $}

\Init{$w = \ind_{\bbR^p} $}

    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $x^{(l)} \leftarrow \argmin \norm{ y -  \mathbf{\Phi} x}_2^2 + \lambda \norm{ w \cdot x}_1$

        Update the weights: for each
        $i = 1, \dots, p, \enspace w_i^{(l+1)} \leftarrow \frac{1}{\lvert x_i^{(l)} \rvert + \epsilon}$
    }

\Return{$x$}
\end{algorithm}
}
\jo{the math seems wrong in the algorithm above. double check it!}

\vskip 0.2in

Note that this algorithm minimizes a concave objective (log-sum is concave), by iteratively solving convex subproblems. Hence, due to the concavity of the objective function, we
are not guaranteed to converge to a global minimum.

\subsection*{Why choosing the log-sum function? How to choose $\epsilon$?}

The log-sum penalty function has the potential to be more sparsity-encouraging than the $l_1$ norm (for a visual explanation, see \cite{Candes_Wakin_Boyd08}). We clearly see
that the log-sum penalty is closer from the $l_0$ norm than the $l_1$ norm thus encouraging smaller coefficients to be set to zero.
\\
More precisely, the smaller $\epsilon$ the closer the log-sum function is from the $l_0$ norm. However, $\epsilon$ can't be set arbitrarily small since as $\epsilon \rightarrow 0$ it
becomes more likely that the iterative reweighted $l_1$ algorithm will be stuck at a local optimum. In practice, note that signal reconstruction is robust to the choice of $\epsilon$.

\section{Multi-Task LASSO}
\label{section_4}

In this section, we assume that $T > 1$. Therefore, we are now predicting a multivariate response $\mathbf{Y}\in \bbR^{n \times T}$, and by assuming that the data collected are linear measurements, the model reads:

\begin{equation*}
    \mathbf{Y} = \mathbf{\Phi}\mathbf{X} + \mathbf{E}
\end{equation*}
\\
where $\mathbf{E} \in \bbR^{n \times T}$ is a matrix of errors (noise), usually simulated with a pre-defined signal-to-noise ratio, when doing compressive sensing. Note that $\mathbf{X} \in \bbR^{p \times T}$ is a matrix.
Like in \nameref{section_2}, suppose that there exists an unknown subset $S \subset  \{1, \dots, p\}$ of the features that are relevant for prediction, and that \textbf{this same subset is preserved across all K components} of the response
variable. In this case, it is natural to consider a group LASSO penalty, in which the $p$ groups are defined by the rows $\{x_{j} \in \bbR^T, j=1,\dots,p\}$ of $\mathbf{X}$.
\\
Therefore, the optimization problem we need to solve is:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \Biggl( \sum_{j=1}^p \norm{ x_{j}}_2 \Biggr)
\end{equation*}
\\
Using the $l_1/l_2$ mixed-norm, it reads:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \norm{ \mathbf{X}}_{2, 1}
\end{equation*}
\\
Note that the fitting term is now a matrix norm (here the Frobenius norm) since we are dealing with matrices. As stated in \nameref{section_2}, we are using a mixed $l_1/l_2$ norm to enforce group sparsity on the coefficients $\mathbf{X}$. Indeed,
we are first computing the $l_2$ norm of every line of the coefficient matrix, then we are computing the $l_1$ norm of the remaining vectors full of the $l_2$ norm of the line of the coefficient matrix.

\subsection*{How is this optimization problem solved?}

In scikit-learn, the algorithm used to fit the model is coordinate descent.

\section{Multi-Task Reweighted LASSO}
\label{section_5}

The Multi-Task Reweighted LASSO is a combination of \nameref{section_3} and \nameref{section_4}. The goal of the model is to predict a multivariate response variable while enforcing a sparser solution than \nameref{section_1}.
\\
To induce a group-sparsity penalty like in \nameref{section_2}, we compute the $l_2$ norm of the lines of the coefficient matrix $\mathbf{X} \in \bbR^{p \times T}$, then we apply like in \nameref{section_3} a weighting matrix
on the resulting vector.
\\
Therefore, the optimization problems reads:

\begin{equation*}
    \mathbf{X}^* = \argmin_{\mathbf{X} \in \bbR^{p \times T}} \frac{1}{2n} \norm{ \mathbf{Y} - \mathbf{\Phi X}}_{\text{F}}^2 + \lambda \sum_{j=1}^p \underbrace{g(\norm{\mathbf{X}_{j:}}_{2})}_{g(x) \leq g(x^{(k)}) + \langle \nabla g (x^{(k)}) , x - x^{(k)} \rangle }
\end{equation*}

where $g$ is any penalty convex or non-convex. In \cite{Candes_Wakin_Boyd08}, $g(x) = \sum_{i=1}^p \log(\lvert x_i \rvert + \epsilon)$. Algorithm 2 shows the log-concave reweighted $l_1$ minimization.

\vskip 0.2in

{\fontsize{4}{4}\selectfont
\begin{algorithm}[h]  % again h stands for here
\SetKwInOut{Input}{input}
\SetKwInOut{Init}{init}
\SetKwInOut{Parameter}{param}
\caption{\textsc{Iterative Multi-task reweighted l1 minimization}
}
%
\Input{$
    \mathbf{\Phi} \in \bbR^{n \times p},
    \mathbf{X} \in \bbR^{p\times T},
    \mathbf{Y} \in \bbR^{n\times T},
    w \in \bbR^{p},
    n_{\text{iter}} \in \bbN,
    \epsilon \in \bbR^+_*,
    \lambda \in \bbR^+
    $}
    \Init{$w = \ind_{\bbR^p} $}

    % Fill  with ones \\
    \For{$l = 0,\dots, n_{\text{iter}}$}
    {
        Solve the weighted $l_1$ minimization problem:
        $\mathbf{X} \leftarrow \argmin \norm{\mathbf{Y} -  \mathbf{\Phi} \mathbf{X}}_{\text{F}}^2 + \lambda \norm{\text{diag}(w) \cdot \mathbf{X}}_{2, 1}$

        Update the weights: for each
        $j = 1, \dots, p, \enspace w_j \leftarrow \frac{1}{\norm{\mathbf{X}_{j:}}_2 + \epsilon}$
    }

\Return{$\mathbf{X}$}
\end{algorithm}
}

\newpage

\section{Bi-level optimization for structured sparse models using SURE}
\label{section_6}

In the following we refer as bi-level optimization the nested optimization problem
which consists in optimizing a criterion (outer loss) with respect to a hyperparameter, subject to
a second optimization problem (inner loss). More formally,

\begin{align*}
    &\lambda^* = \argmin_{\lambda \in \bbR^p} \Biggl\{\mathcal{L}(\lambda) \triangleq \mathcal{C}(\hat{\mathbf{X}}^{(\lambda)})\Biggr\} \\
    &\text{s.t.} \quad \hat{\mathbf{X}}^{(\lambda)} \in \argmin_{\mathbf{X} \in \bbR^{p\times T}} L(\mathbf{X}, \lambda)
\end{align*}

where $\mathcal{L}$ is the outer loss and $L$ is the inner loss.

Let's consider the \nameref{section_4} case. Consider a noisy data generating process such that:

\begin{equation*}
    \mathbf{Y} = \mathbf{\Phi}\mathbf{X}_0 + \mathbf{\Sigma^*}\mathbf{E}
\end{equation*}

where entries of $\mathbf{E}$ are independent, centered and normally distributed and $\mathbf{\Sigma^*} \in \mathbb{S}^n_{++}$ the co-standard-deviation matrix.
For a more formal formulation of the multi-task regression problem, see \cite{Massias_Fercoq_Gramfort_Salmon17}. We wish to estimate
$\mathbf{X}_0$ such that we minimize:

\begin{equation*}
    \text{MSE}(\hat{\mathbf{X}}) = \bbE_{\mathbf{E}}\Biggl[ \norm{\mathbf{X}_0 - \hat{\mathbf{X}}}_{\text{F}}^2 \Biggr]
\end{equation*}

However, the mean squared error can't be evaluated since $\mathbf{E}$ and the noise level
are unknown, thus can't be directly optimized. Therefore, we need to find an estimator of that
quantity that is differentiable (to be optimized using first-order methods). Charles Stein has
proposed an unbiased estimator of the MSE, called the SURE (Stein's unbiased risk estimator) which reads:

\begin{equation*}
    \text{SURE}(\hat{\bf{X}}) = -p\sigma^2 + \norm{\mathbf{Y} - \mathbf{\Phi} \hat{\mathbf{X}}}_{\text{F}}^2 + 2 \sigma^2 \text{df}(\bf{Y})
\end{equation*}

where $\text{df}(\mathbf{Y})$ is the number of degrees of freedom of $\mathbf{Y}$, alternatively called the divergence of
the estimator of $\mathbf{X}_0$. The two first terms are trivially evaluated, while the final term presents some
peculiarities hence the need for Stein's lemma \footnote{For a proof of Stein's lemma, see:
\url{http://www.stat.cmu.edu/~larry/=sml/stein.pdf}}.

As we shall see below, $\text{SURE}$ is a differentiable estimator of $\text{MSE}$ that needs to verify some
non-trivial assumptions, the most notable of which being that \textbf{the estimator must be weakly differentiable with
respect to its target vector}.

\subsection*{Deriving the degrees of freedom in \nameref{section_4}}

Before diving into the derivation of the degrees of freedom in \nameref{section_4}, let's motivate the choice of $\text{SURE}$ as a criterion
to optimize. Remember that in a bi-level optimization setting, we can use $\text{SURE}$ for model selection purposes. LASSO-like
models depend on a tuning parameter $\lambda \in \mathcal{D}$ where $\mathcal{D}$ is some open domain then we can choose this parameter to
minimize $\text{SURE}$:

\begin{equation*}
    \hat{\lambda} = \argmin_{\lambda \in \mathcal{D}} \norm{ \mathbf{Y} - \mathbf{\Phi}\mathbf{X}^{(\lambda)}}_{\text{F}}^2 + 2 \sigma^2 \text{df}(\bf{Y})
\end{equation*}

We will assume that $\mathbf{Y} \sim \mathcal{MN}_{p\times T}(\mathbf{M}, \mathbf{U}, \mathbf{V})$ the matrix Gaussian distribution with location $\mathbf{M}$
and scale parameters $\mathbf{U}$ and $\mathbf{V}$, we consider the lasso fit $\mathbf{\hat{M} = \Phi X}$ as an estimate of $\mathbf{M}$.
\\
\\
First we need to verify that $\hat{\bf{M}}$ is weakly differentiable\footnote{\url{https://en.wikipedia.org/wiki/Weak_derivative}} as a function of $\mathbf{Y}$
(recall that $\hat{\bf{M}}$ depends on $\mathbf{X}$ hence on $\mathbf{Y}$). And before that, we can ask ourselves if this function is even continuous: if
we change $\mathbf{Y}$, the active set of the solution $\mathbf{X}$ can change/jump thus creating discontinuities. Let the support of the solution be $A = \text{supp}(\hat{\mathbf{X}})$.
We can express the LASSO fit as follows:

\begin{equation*}
    \mathbf{\Phi X} = \mathbf{\Phi}_A(\mathbf{\Phi}_A^{\top}\mathbf{\Phi}_A)^{-1}(\mathbf{X}^{\top}_A\mathbf{Y} - \mathbf{\Lambda}\mathbf{S}_A)
\end{equation*}

where $\mathbf{\Phi}_A$ (respectively $\mathbf{X}_A$) indexes the columns of $\bf\Phi$ (respectively $\mathbf{X}$) in $A$. Furthermore, $\mathbf{S}_A$ are the signs of the active
LASSO coefficients and $\bf\Lambda$ is a matrix full of lambda (note that in the special case of \nameref{section_5}, the columns of $\mathbf{\Lambda}$ would be the same but the rows
different).
\\
\\
To prove that this function is continuous, we need to use the dual of the LASSO, which reads:

\begin{align*}
    &\min_{\mathbf{\Theta} \in \bbR^{n \times T}} \norm{ \mathbf{\Theta} - \mathbf{Y} }_{\text{F}}^2 \\
    &\text{s.t.} \quad \norm{\mathbf{\Phi^{\top}\Theta}}_{2, \infty} \leq \lambda
\end{align*}

and the relationship between the dual $\hat{\bf{\Theta}}$ and the primal $\hat{\bf{X}}$ solutions is:

\begin{equation*}
    \bf{\Phi X} = \mathbf{Y - \hat{\Theta}}
\end{equation*}

Yet the dual solution $\hat{\bf{\Theta}}$ is the projection of $\mathbf{Y}$ onto the convex polyhedron $C = \{\bf{\Theta}, \norm{\mathbf{\Phi^{\top}\Theta}}_{2, \infty} \leq \lambda\}$,
which we denote $\hat{\bf{\Theta}} = P_C(\bf{Y})$. Therefore, the \nameref{section_4} fit is simply:

\begin{equation*}
    \mathbf{\Phi X} = (I - P_C)(\mathbf{Y})
\end{equation*}

which is the residual of the projection of $\bfY$ onto $C$.
\\
\\
Yet, we know that a projection onto a convex set is a non-expansive mapping, the same is true for the residual map. It follows:

\begin{equation*}
    \forall \mathbf{Y}, \mathbf{Y'}, \norm{(I - P_C(\mathbf{Y})) - (I - P_C(\mathbf{Y'}))}_{\text{F}} \leq \norm{Y - Y'}_{\text{F}}
\end{equation*}

From the previous equation, we deduce that the LASSO fit is a Lipschitz-continuous mapping, which by Rademacher's theorem, means that it is almost differentiable everywhere.
\\
\\
The continuity can be interpreted as follows: when $\bfY$ changes thus the active set $A$, \textbf{the coefficients of variables to leave
the active continuously drop to zero, and coefficients of variables to enter the active set would continuously move from zero}. This makes the LASSO continuous.


\subsection*{Finding the closed-form expression of SURE for \nameref{section_4}}

Now, we want to derive a closed-form expression of SURE for \nameref{section_4}. Let's re-examine the LASSO fit:

\begin{equation*}
    \mathbf{\Phi X} = \mathbf{\Phi}_A(\mathbf{\Phi}_A^{\top}\mathbf{\Phi}_A)^{-1}(\mathbf{X}^{\top}_A\mathbf{Y} - \mathbf{\Lambda}\mathbf{S}_A)
\end{equation*}

We take the reasonable assumption that $A$ and $\mathbf{S}_A$ are locally constant, meaning that a small change in $\mathbf{Y}$ won't change the active set nor the signs of the active
coefficients. Remember that finding a closed-form expression of SURE boils down to computing the divergence of the LASSO estimator.

\begin{equation*}
    \text{df}(\mathbf{\Phi X}) = \sum_{i=1}^n \sum_{j=1}^T \frac{\partial (\mathbf{\Phi X})_{i,j}}{\partial (\mathbf{Y})_{i, j}} = \text{tr}(\mathbf{\Phi}_A(\mathbf{\Phi}_A^{\top}\mathbf{\Phi}_A)^{-1}\mathbf{\Phi}_A^{\top}) = \lvert A \rvert
\end{equation*}

where $\lvert A \rvert$ is the cardinal of the active set. This yields the following closed-form expression for the risk estimate:

\begin{equation*}
    \text{SURE}(\hat{\bf{X}}) = -p\sigma^2 + \norm{\mathbf{Y} -  \mathbf{\Phi X}}^2_{\text{F}} + 2 \sigma^2 \lvert A \rvert
\end{equation*}

This seems a very reasonable answer since the number of degrees of freedom (number of parameters in a model allowed to change) in LASSO is the active set, meaning the set of non-zero
coefficients.

\newpage
\bibliographystyle{plainnat}
\bibliography{references_all}



\end{document}
